# -*- coding: utf-8 -*-
"""Credit_card.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkRs30fpVUHcIvLH62hHDNU10k8yEkhr

###**Dataset Information**
The dataset contains transactions made by credit cards in September 2013 by European cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.

Update (03/05/2021)
A simulator for transaction data has been released as part of the practical handbook on Machine Learning for Credit Card Fraud Detection - https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_3_GettingStarted/SimulatedDataset.html. We invite all practitioners interested in fraud detection datasets to also check out this data simulator, and the methodologies for credit card fraud detection presented in the book.

###Import libraries
"""

# !pip install tensorflow-gpu==2.9.2

# Commented out IPython magic to ensure Python compatibility.
import keras
keras.__version__
from tensorflow.keras.preprocessing import sequence
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import scipy
import sklearn

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""###Reading dataset"""

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Credit_Card/creditcard.csv")

data.info()

data.shape

# random_state helps assure that you always get the same output when you split the data
# this helps create reproducible results and it does not actually matter what the number is
# frac is percentage of the data that will be returned
data = data.sample(frac = 0.2, random_state = 1)
print(data.shape)

# statistical info
# data.describe()

# check null data
# data.isnull()

"""### Exploratory Data Analisys"""

sns.countplot(data['Class'])

# Plot how fraud and non-fraud cases are scattered 
plt.scatter(data.loc[data['Class'] == 0]['V1'], data.loc[data['Class'] == 0]['V2'], label="Class #0", alpha=0.5, linewidth=0.15)
plt.scatter(data.loc[data['Class'] == 1]['V1'], data.loc[data['Class'] == 1]['V2'], label="Class #1", alpha=0.5, linewidth=0.15,c='r')
plt.show()

# plot the histogram of each parameter
data.hist(figsize = (20, 20))
plt.show()

fraud = data.loc[data['Class'] == 1]
normal = data.loc[data['Class'] == 0]

print(len(fraud))
print(len(normal))

from sklearn import linear_model
from sklearn.model_selection import train_test_split

X = data.iloc[:, :-1]
y = data['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35)

clf = linear_model.LogisticRegression(C=1e5)

clf.fit(X_train, y_train)

y_pred = np.array(clf.predict(X_test))
y = np.array(y_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

print(confusion_matrix(y, y_pred))

print(accuracy_score(y, y_pred))

print(classification_report(y, y_pred))

# correlation matrix
corrmat = data.corr()
fig = plt.figure(figsize = (12, 9))

sns.heatmap(corrmat, vmax = .8, square = True)
plt.show()

"""###Transaction Amount Visualization

Expect a lot of low-value transactions to be uninteresting (buying cups of coffee, lunches, etc).


Only visualizes the transactions between USD 200 and 2000.



"""

# Plot of high value transactions($200-$2000)
bins = np.linspace(200, 2000, 100)
plt.hist(normal.Amount, bins, alpha=1, normed=True, label='Non-Fraud')
plt.hist(fraud.Amount, bins, alpha=1, normed=True, label='Fraud')
plt.legend(loc='upper right')
plt.title("Amount by percentage of transactions (transactions \$200-$2000)")
plt.xlabel("Transaction amount (USD)")
plt.ylabel("Percentage of transactions (%)")
plt.show()

# Batch size, nr tranzactii, features -> inputul retelei
# Total linii, Nr tranzactii, features -> preprocesarea setului de date

def generator(data, lookback, delay, min_index, max_index,
              shuffle=False, batch_size=128, step=1):
    if max_index is None:
        max_index = len(data) - delay - 1
    i = min_index + lookback
    while 1:
        if shuffle:
            rows = np.random.randint(
                min_index + lookback, max_index, size=batch_size)
        else:
            if i + batch_size >= max_index:
                i = min_index + lookback
            rows = np.arange(i, min(i + batch_size, max_index))
            i += len(rows)

        samples = np.zeros((len(rows),
                           lookback // step,
                           data.shape[-1]))
        targets = np.zeros((len(rows),))
        for j, row in enumerate(rows):
            indices = range(rows[j] - lookback, rows[j], step)
            samples[j] = data.iloc[indices]
            targets[j] = data.iloc[rows[j] + delay]["Class"]
        yield samples, targets

lookback = 10 # pe baza a 10 tranzactii o determin pe a 11-a
step = 1
delay = 0
batch_size = 10000

train_gen = generator(data,
                      lookback=lookback,
                      delay=delay,
                      min_index=0,
                      max_index=200000,
                      shuffle=True,
                      step=step, 
                      batch_size=batch_size)
val_gen = generator(data,
                    lookback=lookback,
                    delay=delay,
                    min_index=189873,
                    max_index=234563,
                    step=step,
                    batch_size=batch_size)
test_gen = generator(data,
                     lookback=lookback,
                     delay=delay,
                     min_index=234564,
                     max_index=None,
                     step=step,
                     batch_size=batch_size)

# Number of steps to retrieve data from the val_gen object
# ensuring the processing of the entire validation data set.
val_steps = (300000 - 200001 - lookback) // batch_size

# Number of steps to retrieve data from the test_gen object, 
# at which the entire test data set will be processed.
test_steps = (len(data) - 300001 - lookback) // batch_size

train_gen.__next__()

from keras.models import Sequential
from keras import layers
from tensorflow.keras.optimizers import RMSprop
from keras.layers import SimpleRNN

model = Sequential()
# model.add(layers.Flatten(input_shape=(lookback // step, data.shape[-1])))
model.add(SimpleRNN(32))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=RMSprop(), loss='binary_crossentropy', metrics=["accuracy"])
history = model.fit_generator(train_gen,
                              steps_per_epoch=100,
                              epochs=20,
                              validation_data=val_gen,
                              validation_steps=val_steps)

